


import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from statsmodels.api import Logit, add_constant
import statsmodels.api as sm

# Load the dataset
merged_df = pd.read_csv('merged_df.csv')

# Display the first few rows of the dataset to ensure it loaded correctly
print(merged_df.head())



import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from statsmodels.api import Logit, add_constant
import statsmodels.api as sm


# Identify and drop any non-numeric columns if they exist
# Assuming 'outcome' is the only non-numeric column needed
non_numeric_cols = merged_df.select_dtypes(include=['object', 'datetime']).columns.tolist()
print(f"Non-numeric columns to be dropped: {non_numeric_cols}")
data = merged_df.drop(columns=non_numeric_cols)

# Separate the outcome variable
outcome = data['outcome']
data = data.drop(columns=['outcome'])

# Ensure all remaining columns are numeric
data = data.apply(pd.to_numeric, errors='coerce')

# Remove columns with NA values that could not be converted to numeric
data = data.dropna(axis=1, how='any')

# Standardize the data
scaler = StandardScaler()
standardized_data = scaler.fit_transform(data)

# Remove columns with zero variance
data_var = pd.DataFrame(standardized_data, columns=data.columns)
data_var = data_var.loc[:, (data_var != data_var.iloc[0]).any()]

# Verify that there are no zero variance columns
print(f"Columns retained after removing zero variance columns: {data_var.shape[1]}")

# Apply PCA
pca = PCA()
pc_data = pca.fit_transform(data_var)

# Check explained variance ratio
explained_variance = pca.explained_variance_ratio_
print(f"Explained variance ratio of each principal component: {explained_variance}")

# Convert to DataFrame for ease of use
pc_df = pd.DataFrame(pc_data, columns=[f'PC{i+1}' for i in range(pc_data.shape[1])])

# Combine principal components with the outcome variable
pca_data = pd.concat([pc_df, outcome.reset_index(drop=True)], axis=1)

# Fit the full logistic regression model using principal components
X = add_constant(pca_data.drop(columns=['outcome']))
y = pca_data['outcome']

# Fit the model
full_model = Logit(y, X).fit()
print(full_model.summary())

# Perform stepwise selection based on AIC
def stepwise_selection(X, y):
    initial_list = X.columns.tolist()
    best_model = Logit(y, X).fit()
    best_aic = best_model.aic
    improved = True
    
    while improved:
        improved = False
        results = []
        
        # Try dropping each column
        for column in initial_list:
            temp_X = X.drop(columns=[column])
            model = Logit(y, temp_X).fit(disp=0)
            results.append((model.aic, column, model))
        
        # Try adding each column
        for column in [col for col in data.columns if col not in initial_list]:
            temp_X = X.join(data[column])
            model = Logit(y, temp_X).fit(disp=0)
            results.append((model.aic, column, model))
        
        results.sort()
        best_aic, best_column, best_model = results[0]
        
        if best_aic < best_model.aic:
            improved = True
            initial_list = best_model.params.index.tolist()
    
    return best_model

best_model = stepwise_selection(X, y)
print(best_model.summary())



import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, StratifiedKFold
import numpy as np


# Identify and drop any non-numeric columns if they exist
non_numeric_cols = merged_df.select_dtypes(include=['object', 'datetime']).columns.tolist()
data = merged_df.drop(columns=non_numeric_cols)

# Separate the outcome variable
outcome = data['outcome']
data = data.drop(columns=['outcome'])

# Ensure all remaining columns are numeric
data = data.apply(pd.to_numeric, errors='coerce')

# Remove columns with NA values that could not be converted to numeric
data = data.dropna(axis=1, how='any')

# Standardize the data
scaler = StandardScaler()
standardized_data = scaler.fit_transform(data)

# Remove columns with zero variance
data_var = pd.DataFrame(standardized_data, columns=data.columns)
data_var = data_var.loc[:, (data_var != data_var.iloc[0]).any()]

# Apply PCA
pca = PCA(n_components=15)  # Use fewer components to avoid overfitting and convergence issues
pc_data = pca.fit_transform(data_var)

# Check explained variance ratio
explained_variance = pca.explained_variance_ratio_
print(f"Explained variance ratio of each principal component: {explained_variance}")

# Convert to DataFrame for ease of use
pc_df = pd.DataFrame(pc_data, columns=[f'PC{i+1}' for i in range(pc_data.shape[1])])

# Combine principal components with the outcome variable
pca_data = pd.concat([pc_df, outcome.reset_index(drop=True)], axis=1)

# Define feature matrix X and target vector y
X = pca_data.drop(columns=['outcome'])
y = pca_data['outcome']

# Initialize the logistic regression model
log_reg = LogisticRegression(max_iter=1000)

# Perform cross-validation
kf = StratifiedKFold(n_splits=5)
scores = cross_val_score(log_reg, X, y, cv=kf, scoring='accuracy')
print(f'Cross-validated accuracy: {np.mean(scores)}')

# Fit the logistic regression model
log_reg.fit(X, y)

# Print the coefficients
print("Coefficients:", log_reg.coef_)
print("Intercept:", log_reg.intercept_)



import matplotlib.pyplot as plt

# Plot explained variance ratio
plt.figure(figsize=(10, 6))
#bars
plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, align='center')
#red line
plt.step(range(1, len(explained_variance) + 1), np.cumsum(explained_variance), where='mid', linestyle='--', color='red')
plt.xlabel('Principal Components')
plt.ylabel('Explained Variance Ratio')
plt.title('Explained Variance Ratio of Principal Components')
plt.show()



import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, StratifiedKFold
import matplotlib.pyplot as plt



# Identify and drop any non-numeric columns if they exist
non_numeric_cols = merged_df.select_dtypes(include=['object', 'datetime']).columns.tolist()
data = merged_df.drop(columns=non_numeric_cols)

# Separate the outcome variable
outcome = data['outcome']
data = data.drop(columns=['outcome'])

# Ensure all remaining columns are numeric
data = data.apply(pd.to_numeric, errors='coerce')

# Remove columns with NA values that could not be converted to numeric
data = data.dropna(axis=1, how='any')

# Standardize the data
scaler = StandardScaler()
standardized_data = scaler.fit_transform(data)

# Remove columns with zero variance
data_var = pd.DataFrame(standardized_data, columns=data.columns)
data_var = data_var.loc[:, (data_var != data_var.iloc[0]).any()]

# Apply PCA
pca = PCA(n_components=15)  # Use fewer components to avoid overfitting and convergence issues
pc_data = pca.fit_transform(data_var)

# Check explained variance ratio
explained_variance = pca.explained_variance_ratio_
print(f"Explained variance ratio of each principal component: {explained_variance}")

# Convert to DataFrame for ease of use
pc_df = pd.DataFrame(pc_data, columns=[f'PC{i+1}' for i in range(pc_data.shape[1])])

# Combine principal components with the outcome variable
pca_data = pd.concat([pc_df, outcome.reset_index(drop=True)], axis=1)

# Define feature matrix X and target vector y
X = pca_data.drop(columns=['outcome'])
y = pca_data['outcome']

# Initialize the logistic regression model
log_reg = LogisticRegression(max_iter=1000)

# Perform cross-validation
kf = StratifiedKFold(n_splits=5)
scores = cross_val_score(log_reg, X, y, cv=kf, scoring='accuracy')
print(f'Cross-validated accuracy: {np.mean(scores)}')

# Fit the logistic regression model
log_reg.fit(X, y)

# Print the coefficients
print("Coefficients:", log_reg.coef_)
print("Intercept:", log_reg.intercept_)

# Calculate the loadings
loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i+1}' for i in range(pc_data.shape[1])], index=data.columns)
print(loadings)

# Identify the most influential features in each principal component
top_features_per_pc = {}
for i in range(1, 16):
    pc_loadings = loadings[f'PC{i}'].sort_values(ascending=False)
    top_features_per_pc[f'PC{i}'] = pc_loadings.index[:5].tolist()  # Top 5 features for each PC

print("Top features per principal component:")
for pc, features in top_features_per_pc.items():
    print(f"{pc}: {features}")





import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Assuming merged_df is your DataFrame
# merged_df = pd.read_csv('your_data.csv')  # Uncomment this if you need to load your data

# List of columns to exclude (e.g., identifiers)
exclude_columns = ['subject_id']

# Filter out non-numeric columns and exclude specific columns
numeric_df = merged_df.select_dtypes(include=[np.number]).drop(columns=exclude_columns)

# Impute missing values
imputer = SimpleImputer(strategy='mean')  # You can also use 'median' or other strategies
numeric_df_imputed = imputer.fit_transform(numeric_df)

# Standardize the data (optional but recommended for PCA)
scaler = StandardScaler()
data_scaled = scaler.fit_transform(numeric_df_imputed)

# Perform PCA
pca = PCA()
pca.fit(data_scaled)

# Get explained variance ratio and PCA components
explained_variance_ratio = pca.explained_variance_ratio_
components = pca.components_

# Calculate the cumulative explained variance to decide the number of components to retain
cumulative_variance = np.cumsum(explained_variance_ratio)
num_components = np.argmax(cumulative_variance >= 0.70) + 1  # Retain components that explain at least 70% variance

# Create a DataFrame to store the feature loadings
loadings = pd.DataFrame(components.T, columns=[f'PC{i+1}' for i in range(len(components))], index=numeric_df.columns)

# Sum the absolute values of the loadings for the retained components
top_loadings = loadings.iloc[:, :num_components].abs().sum(axis=1)

x = 10

# Select the top features based on their summed loadings
top_features = top_loadings.nlargest(x).index.tolist()  # Change 15 to however many top features you want to select

print("Top", x,  "features selected based on PCA loadings:", top_features)







