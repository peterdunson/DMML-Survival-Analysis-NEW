





import pandas as pd
import numpy as np
from cmdstanpy import CmdStanModel
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import arviz as az
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv("RF_imputation_NEW.csv")

# Drop unnecessary columns
data.drop(columns=['deathtime', 'survival_time', 'LOS', 'Unnamed_0', 'V1', 'admittime', 'ID', 'group', 'tLOS', 'subject_id', 'COPD', 'CHD_with_no_MI'], inplace=True)

# Ensure the 'outcome' column is present and clean
if 'outcome' not in data.columns:
    raise ValueError("The 'outcome' column does not exist in the dataframe.")
if not data['outcome'].isin([0, 1]).all():
    raise ValueError("The 'outcome' column contains values other than 0 and 1.")

# Convert 'outcome' to integers
data['outcome'] = data['outcome'].astype(int)

# Normalize the predictors
predictor_names = data.columns.difference(['outcome'])
scaler = StandardScaler()
data[predictor_names] = scaler.fit_transform(data[predictor_names])

# Split the data into Training and Validation Sets
train_data, valid_data = train_test_split(data, test_size=0.3, random_state=532)

# Prepare data for Stan model
stan_data = {
    'N': train_data.shape[0],
    'K': len(predictor_names),
    'X': train_data[predictor_names].values,
    'y': train_data['outcome'].values
}

# Compile Stan model
model = CmdStanModel(stan_file='logistic_regression.stan')

# Sample from the posterior
fit = model.sample(data=stan_data, seed=532, chains=4, parallel_chains=4, iter_sampling=2000, iter_warmup=1000)

# Extract and summarize all coefficients
beta_summary = az.summary(fit, var_names=['beta'], round_to=2)
beta_summary['Feature'] = predictor_names

print(beta_summary[['Feature', 'mean', 'sd', 'hdi_3%', 'hdi_97%', 'r_hat']])

# To visualize the coefficients
az.plot_trace(fit, var_names=['beta'])
plt.show()

# Calculate the mean and HDI (Highest Density Interval) for the coefficients
beta_samples = fit.stan_variable('beta')
alpha_samples = fit.stan_variable('alpha')
beta_means = beta_samples.mean(axis=0)
beta_hdi = np.percentile(beta_samples, [2.5, 97.5], axis=0)

# Selected features based on HDI not including zero
selected_features = predictor_names[(beta_hdi[0] > 0) | (beta_hdi[1] < 0)]
print("Selected features:", selected_features)

# Posterior predictive checks
y_pred_train = fit.stan_variable('y_pred').mean(axis=0)
train_auc_value = roc_auc_score(train_data['outcome'], y_pred_train)
print("Train AUC:", train_auc_value)

# Prepare validation data for prediction
stan_data_valid = {
    'N': valid_data.shape[0],
    'K': len(predictor_names),
    'X': valid_data[predictor_names].values,
    'y': valid_data['outcome'].values
}

# Generate predictions for validation set
fit_valid = model.sample(data=stan_data_valid, seed=532, chains=4, parallel_chains=4, iter_sampling=2000, iter_warmup=1000)

# Extract and calculate validation predictions
y_pred_valid = fit_valid.stan_variable('y_pred').mean(axis=0)
test_auc_value = roc_auc_score(valid_data['outcome'], y_pred_valid)
print("Test AUC:", test_auc_value)









import pandas as pd
import numpy as np
from cmdstanpy import CmdStanModel
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import arviz as az

# Load the data
data = pd.read_csv("RF_imputation_NEW.csv")

# Drop unnecessary columns
data.drop(columns=['deathtime', 'survival_time', 'LOS', 'Unnamed_0', 'V1', 'admittime', 'ID', 'group', 'tLOS', 'subject_id'], inplace=True)

# Ensure the 'outcome' column is present and clean
if 'outcome' not in data.columns:
    raise ValueError("The 'outcome' column does not exist in the dataframe.")
if not data['outcome'].isin([0, 1]).all():
    raise ValueError("The 'outcome' column contains values other than 0 and 1.")

# Convert 'outcome' to integers
data['outcome'] = data['outcome'].astype(int)

# Normalize the predictors
predictor_names = data.columns.difference(['outcome'])
scaler = StandardScaler()
data[predictor_names] = scaler.fit_transform(data[predictor_names])

# Split the data into Training and Validation Sets
train_data, valid_data = train_test_split(data, test_size=0.3, random_state=532)

# Prepare data for Stan model
stan_data = {
    'N': train_data.shape[0],
    'K': len(predictor_names),
    'X': train_data[predictor_names].values,
    'y': train_data['outcome'].values
}

# Compile Stan model
model = CmdStanModel(stan_file='logistic_regression.stan')

# Sample from the posterior
fit = model.sample(data=stan_data, seed=213, chains=4, parallel_chains=4, iter_sampling=2000, iter_warmup=1000)

# Extract and summarize all coefficients
beta_summary = az.summary(fit, var_names=['beta'], round_to=2)
beta_summary['Feature'] = predictor_names

# Print the summary of all features
print(beta_summary[['Feature', 'mean', 'sd', 'hdi_3%', 'hdi_97%', 'r_hat']])

# Calculate the mean and HDI (Highest Density Interval) for the coefficients
beta_samples = fit.stan_variable('beta')
beta_hdi = np.percentile(beta_samples, [2.5, 97.5], axis=0)

# Selected features based on HDI not including zero
selected_features = predictor_names[(beta_hdi[0] > 0) | (beta_hdi[1] < 0)]
print("Selected features:", selected_features)

# Posterior predictive checks
y_pred_train = fit.stan_variable('y_pred').mean(axis=0)
train_auc_value = roc_auc_score(train_data['outcome'], y_pred_train)
print("Train AUC:", train_auc_value)

# Prepare validation data for prediction
stan_data_valid = {
    'N': valid_data.shape[0],
    'K': len(predictor_names),
    'X': valid_data[predictor_names].values,
    'y': valid_data['outcome'].values
}

# Generate predictions for validation set
fit_valid = model.sample(data=stan_data_valid, seed=532, chains=4, parallel_chains=4, iter_sampling=2000, iter_warmup=1000)

# Extract and calculate validation predictions
y_pred_valid = fit_valid.stan_variable('y_pred').mean(axis=0)
test_auc_value = roc_auc_score(valid_data['outcome'], y_pred_valid)
print("Test AUC:", test_auc_value)





















import pandas as pd
import numpy as np
from cmdstanpy import CmdStanModel
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import arviz as az

# Load and preprocess data
data = pd.read_csv("RF_imputation_NEW.csv")
data.drop(columns=['deathtime', 'survival_time', 'LOS', 'Unnamed_0', 'V1', 'admittime', 'ID', 'group', 'tLOS', 'subject_id', 'COPD', 'CHD_with_no_MI'], inplace=True)
data['outcome'] = data['outcome'].astype(int)
predictor_names = data.columns.difference(['outcome'])
scaler = StandardScaler()
data[predictor_names] = scaler.fit_transform(data[predictor_names])
train_data, valid_data = train_test_split(data, test_size=0.3, random_state=213)

# Prepare data for Stan model
stan_data = {
    'N': train_data.shape[0],
    'K': len(predictor_names),
    'X': train_data[predictor_names].values,
    'y': train_data['outcome'].values
}

# Compile and sample from Stan model
model = CmdStanModel(stan_file='logistic_regression.stan')
fit = model.sample(data=stan_data, seed=213, chains=4, parallel_chains=4, iter_sampling=2000, iter_warmup=1000)

# Convert the CmdStanPy output to ArviZ's InferenceData object
idata = az.from_cmdstanpy(fit)

# Check MCMC diagnostics for convergence and mixing
print("MCMC diagnostics:")
print(az.summary(idata, var_names=['beta']))  # Summary statistics
print("R-hat diagnostic:")
print(az.rhat(idata))  # R-hat diagnostic
print("Effective sample size (ESS):")
print(az.ess(idata))  # Effective sample size

# Extract and summarize all coefficients
beta_summary = az.summary(idata, var_names=['beta'], round_to=2)
beta_summary['Feature'] = predictor_names
print(beta_summary[['Feature', 'mean', 'sd', 'hdi_3%', 'hdi_97%', 'r_hat']])

# Calculate HDI for coefficients
beta_samples = fit.stan_variable('beta')
beta_hdi = np.percentile(beta_samples, [2.5, 97.5], axis=0)

# Identify significant features based on HDI not including zero
significant_features = predictor_names[(beta_hdi[0] > 0) | (beta_hdi[1] < 0)]
print("Significant features (HDI not including zero):")
print(significant_features)

# Compute AUC for training set
y_pred_train = fit.stan_variable('y_pred').mean(axis=0)
train_auc_value = roc_auc_score(train_data['outcome'], y_pred_train)
print("Train AUC:", train_auc_value)

# Prepare validation data and compute AUC for test set
stan_data_valid = {
    'N': valid_data.shape[0],
    'K': len(predictor_names),
    'X': valid_data[predictor_names].values,
    'y': valid_data['outcome'].values
}
fit_valid = model.sample(data=stan_data_valid, seed=213, chains=4, parallel_chains=4, iter_sampling=2000, iter_warmup=1000)
y_pred_valid = fit_valid.stan_variable('y_pred').mean(axis=0)
test_auc_value = roc_auc_score(valid_data['outcome'], y_pred_valid)
print("Test AUC:", test_auc_value)






import pandas as pd
import numpy as np
from cmdstanpy import CmdStanModel
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import arviz as az

# Load and preprocess data
data = pd.read_csv("RF_imputation_NEW.csv")
data.drop(columns=['deathtime', 'survival_time', 'LOS', 'Unnamed_0', 'V1', 'admittime', 'ID', 'group', 'tLOS', 'subject_id', 'COPD', 'CHD_with_no_MI'], inplace=True)
data['outcome'] = data['outcome'].astype(int)
predictor_names = data.columns.difference(['outcome'])
scaler = StandardScaler()
data[predictor_names] = scaler.fit_transform(data[predictor_names])
train_data, valid_data = train_test_split(data, test_size=0.3, random_state=213)

# Prepare data for Stan model
stan_data = {
    'N': train_data.shape[0],
    'K': len(predictor_names),
    'X': train_data[predictor_names].values,
    'y': train_data['outcome'].values
}

# Compile and sample from Stan model
model = CmdStanModel(stan_file='logistic_regression_horseshoe.stan')
fit = model.sample(data=stan_data, seed=213, chains=4, parallel_chains=4, iter_sampling=2000, iter_warmup=1000)

# Convert the CmdStanPy output to ArviZ's InferenceData object
idata = az.from_cmdstanpy(fit)

# Check MCMC diagnostics for convergence and mixing
print("MCMC diagnostics:")
print(az.summary(idata, var_names=['beta']))  # Summary statistics
print("R-hat diagnostic:")
print(az.rhat(idata))  # R-hat diagnostic
print("Effective sample size (ESS):")
print(az.ess(idata))  # Effective sample size

# Extract and summarize all coefficients
beta_summary = az.summary(idata, var_names=['beta'], round_to=2)
beta_summary['Feature'] = predictor_names
print(beta_summary[['Feature', 'mean', 'sd', 'hdi_3%', 'hdi_97%', 'r_hat']])

# Calculate HDI for coefficients
beta_samples = fit.stan_variable('beta')
beta_hdi = np.percentile(beta_samples, [2.5, 97.5], axis=0)

# Identify significant features based on HDI not including zero
significant_features = predictor_names[(beta_hdi[0] > 0) | (beta_hdi[1] < 0)]
print("Significant features (HDI not including zero):")
print(significant_features)

# Compute AUC for training set
y_pred_train = fit.stan_variable('y_pred').mean(axis=0)
train_auc_value = roc_auc_score(train_data['outcome'], y_pred_train)
print("Train AUC:", train_auc_value)

# Prepare validation data and compute AUC for test set
stan_data_valid = {
    'N': valid_data.shape[0],
    'K': len(predictor_names),
    'X': valid_data[predictor_names].values,
    'y': valid_data['outcome'].values
}
fit_valid = model.sample(data=stan_data_valid, seed=213, chains=4, parallel_chains=4, iter_sampling=2000, iter_warmup=1000)
y_pred_valid = fit_valid.stan_variable('y_pred').mean(axis=0)
test_auc_value = roc_auc_score(valid_data['outcome'], y_pred_valid)
print("Test AUC:", test_auc_value)










