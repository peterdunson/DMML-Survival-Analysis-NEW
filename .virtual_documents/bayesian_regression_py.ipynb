





import pandas as pd
import bambi as bmb
import arviz as az
from sklearn.metrics import roc_auc_score

# Load the data
data = pd.read_csv('RF_imputation_NEW.csv')

# Drop unwanted columns
data = data.drop(columns=['deathtime', 'survival_time', 'LOS', 'Unnamed_0', 'V1', 'admittime', 'ID', 'group', 'tLOS', 'subject_id'])

# Normalize the predictors
predictor_columns = data.columns.difference(['outcome'])
data[predictor_columns] = (data[predictor_columns] - data[predictor_columns].mean()) / data[predictor_columns].std()

# Split the data into training and validation sets
train_data = data.sample(frac=0.7, random_state=213)
valid_data = data.drop(train_data.index)

# Define and fit the Bayesian logistic regression model using Laplace approximation
model = bmb.Model('outcome ~ ' + ' + '.join(predictor_columns), train_data, family='bernoulli')
fitted_model = model.fit(inference_method="laplace")

# Summarize the model
print(fitted_model.summary())

# Evaluate the model performance on the training set
train_preds_prob = fitted_model.predict(train_data)
train_auc_value = roc_auc_score(train_data['outcome'], train_preds_prob)
print("Train AUC:", train_auc_value)

# Evaluate the model performance on the validation set
valid_preds_prob = fitted_model.predict(valid_data)
valid_auc_value = roc_auc_score(valid_data['outcome'], valid_preds_prob)
print("Validation AUC:", valid_auc_value)




import pandas as pd
import numpy as np
from cmdstanpy import CmdStanModel
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import arviz as az
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv("RF_imputation_NEW.csv")

# Drop unnecessary columns
data.drop(columns=['deathtime', 'survival_time', 'LOS', 'Unnamed_0', 'V1', 'admittime', 'ID', 'group', 'tLOS', 'subject_id', 'COPD', 'CHD_with_no_MI'], inplace=True)

# Ensure the 'outcome' column is present and clean
if 'outcome' not in data.columns:
    raise ValueError("The 'outcome' column does not exist in the dataframe.")
if not data['outcome'].isin([0, 1]).all():
    raise ValueError("The 'outcome' column contains values other than 0 and 1.")

# Convert 'outcome' to integers
data['outcome'] = data['outcome'].astype(int)

# Normalize the predictors
predictor_names = data.columns.difference(['outcome'])
scaler = StandardScaler()
data[predictor_names] = scaler.fit_transform(data[predictor_names])

# Split the data into Training and Validation Sets
train_data, valid_data = train_test_split(data, test_size=0.3, random_state=532)

# Prepare data for Stan model
stan_data = {
    'N': train_data.shape[0],
    'K': len(predictor_names),
    'X': train_data[predictor_names].values,
    'y': train_data['outcome'].values
}

# Compile Stan model
model = CmdStanModel(stan_file='logistic_regression.stan')

# Sample from the posterior
fit = model.sample(data=stan_data, seed=532, chains=4, parallel_chains=4, iter_sampling=2000, iter_warmup=1000)

# Extract and summarize all coefficients
beta_summary = az.summary(fit, var_names=['beta'], round_to=2)
beta_summary['Feature'] = predictor_names

print(beta_summary[['Feature', 'mean', 'sd', 'hdi_3%', 'hdi_97%', 'r_hat']])

# To visualize the coefficients
az.plot_trace(fit, var_names=['beta'])
plt.show()

# Calculate the mean and HDI (Highest Density Interval) for the coefficients
beta_samples = fit.stan_variable('beta')
alpha_samples = fit.stan_variable('alpha')
beta_means = beta_samples.mean(axis=0)
beta_hdi = np.percentile(beta_samples, [2.5, 97.5], axis=0)

# Selected features based on HDI not including zero
selected_features = predictor_names[(beta_hdi[0] > 0) | (beta_hdi[1] < 0)]
print("Selected features:", selected_features)

# Posterior predictive checks
y_pred_train = fit.stan_variable('y_pred').mean(axis=0)
train_auc_value = roc_auc_score(train_data['outcome'], y_pred_train)
print("Train AUC:", train_auc_value)

# Prepare validation data for prediction
stan_data_valid = {
    'N': valid_data.shape[0],
    'K': len(predictor_names),
    'X': valid_data[predictor_names].values,
    'y': valid_data['outcome'].values
}

# Generate predictions for validation set
fit_valid = model.sample(data=stan_data_valid, seed=532, chains=4, parallel_chains=4, iter_sampling=2000, iter_warmup=1000)

# Extract and calculate validation predictions
y_pred_valid = fit_valid.stan_variable('y_pred').mean(axis=0)
test_auc_value = roc_auc_score(valid_data['outcome'], y_pred_valid)
print("Test AUC:", test_auc_value)



import pandas as pd
import numpy as np
from cmdstanpy import CmdStanModel
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import arviz as az

# Load the data
data = pd.read_csv("RF_imputation_NEW.csv")

# Drop unnecessary columns
data.drop(columns=['deathtime', 'survival_time', 'LOS', 'Unnamed_0', 'V1', 'admittime', 'ID', 'group', 'tLOS', 'subject_id'], inplace=True)

# Ensure the 'outcome' column is present and clean
if 'outcome' not in data.columns:
    raise ValueError("The 'outcome' column does not exist in the dataframe.")
if not data['outcome'].isin([0, 1]).all():
    raise ValueError("The 'outcome' column contains values other than 0 and 1.")

# Convert 'outcome' to integers
data['outcome'] = data['outcome'].astype(int)

# Normalize the predictors
predictor_names = data.columns.difference(['outcome'])
scaler = StandardScaler()
data[predictor_names] = scaler.fit_transform(data[predictor_names])

# Split the data into Training and Validation Sets
train_data, valid_data = train_test_split(data, test_size=0.3, random_state=532)

# Prepare data for Stan model
stan_data = {
    'N': train_data.shape[0],
    'K': len(predictor_names),
    'X': train_data[predictor_names].values,
    'y': train_data['outcome'].values
}

# Compile Stan model
model = CmdStanModel(stan_file='logistic_regression.stan')

# Sample from the posterior
fit = model.sample(data=stan_data, seed=213, chains=4, parallel_chains=4, iter_sampling=2000, iter_warmup=1000)

# Extract and summarize all coefficients
beta_summary = az.summary(fit, var_names=['beta'], round_to=2)
beta_summary['Feature'] = predictor_names

# Print the summary of all features
print(beta_summary[['Feature', 'mean', 'sd', 'hdi_3%', 'hdi_97%', 'r_hat']])

# Calculate the mean and HDI (Highest Density Interval) for the coefficients
beta_samples = fit.stan_variable('beta')
beta_hdi = np.percentile(beta_samples, [2.5, 97.5], axis=0)

# Selected features based on HDI not including zero
selected_features = predictor_names[(beta_hdi[0] > 0) | (beta_hdi[1] < 0)]
print("Selected features:", selected_features)

# Posterior predictive checks
y_pred_train = fit.stan_variable('y_pred').mean(axis=0)
train_auc_value = roc_auc_score(train_data['outcome'], y_pred_train)
print("Train AUC:", train_auc_value)

# Prepare validation data for prediction
stan_data_valid = {
    'N': valid_data.shape[0],
    'K': len(predictor_names),
    'X': valid_data[predictor_names].values,
    'y': valid_data['outcome'].values
}

# Generate predictions for validation set
fit_valid = model.sample(data=stan_data_valid, seed=532, chains=4, parallel_chains=4, iter_sampling=2000, iter_warmup=1000)

# Extract and calculate validation predictions
y_pred_valid = fit_valid.stan_variable('y_pred').mean(axis=0)
test_auc_value = roc_auc_score(valid_data['outcome'], y_pred_valid)
print("Test AUC:", test_auc_value)







