---
title: "random_forest"
author: "Peter"
date: "`r Sys.Date()`"
output: html_document
---






```{r}
# Load necessary libraries
library(data.table)
library(randomForest)
library(caret)
library(pROC)

# Load the dataset
RF_impute_df <- fread("RF_imputation_NEW.csv")

# Subset the data to exclude unnecessary columns
RF_complete_df <- subset(RF_impute_df, select = -c(deathtime, survival_time, LOS, Unnamed_0, V1, admittime, ID, group, tLOS))

# Ensure the outcome variable is included and properly named
if (!("outcome" %in% names(RF_complete_df))) {
  stop("The 'outcome' column does not exist in the dataframe.")
}

# Ensure the outcome variable has valid factor levels
RF_complete_df$outcome <- factor(RF_complete_df$outcome, levels = unique(RF_complete_df$outcome), labels = make.names(unique(RF_complete_df$outcome)))

# Print the column names to verify
cat("Column names in the dataset:\n")
print(names(RF_complete_df))

# Prepare the predictor matrix and outcome variable
x_rf <- as.matrix(RF_complete_df[, setdiff(names(RF_complete_df), "outcome"), with = FALSE])
y_rf <- RF_complete_df$outcome

# Set seed for reproducibility
set.seed(510)

# Define cross-validation control
train_control <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = "final"
)

# Define the grid for hyperparameter tuning
tune_grid <- expand.grid(
  mtry = 10  # Number of variables randomly sampled as candidates at each split
)

# Train the Random Forest model using cross-validation with the specified parameters
rf_cv_model <- train(
  x = x_rf, 
  y = y_rf,
  method = "rf",
  trControl = train_control,
  tuneGrid = tune_grid,
  metric = "ROC",
  importance = TRUE,
  ntree = 500,  # Number of trees
  nodesize = 10  # Minimum size of terminal nodes
)

# Print the results of the cross-validation
print(rf_cv_model)

# Extract the cross-validated AUC
auc <- rf_cv_model$results$ROC[rf_cv_model$results$ROC == max(rf_cv_model$results$ROC)]
cat("Cross-validated AUC:", auc, "\n")

# Predict on the test data (if you have a separate test set)
# Here we use the cross-validated predictions
predictions <- rf_cv_model$pred$pred
probs <- rf_cv_model$pred[, make.names(levels(y_rf)[2])]  # Assuming the positive class is the second level

# Calculate log-loss
log_loss <- function(y_true, y_prob) {
  epsilon <- 1e-15
  y_prob <- pmax(epsilon, pmin(1 - epsilon, y_prob))
  -mean(y_true * log(y_prob) + (1 - y_true) * log(1 - y_prob))
}

# Convert factor levels to 0 and 1 for log-loss calculation
y_true <- ifelse(rf_cv_model$pred$obs == levels(y_rf)[2], 1, 0)
log_loss_value <- log_loss(y_true, probs)
cat("Cross-validated log-loss:", log_loss_value, "\n")

# Extract feature importances correctly
importance_rf <- varImp(rf_cv_model, scale = FALSE)
importance_values <- importance_rf$importance[, 1]  # Extract importance values

# Print feature importances
cat("Feature importances from Random Forest (cross-validated):\n")
print(importance_values)

# Plot feature importances
plot(importance_rf, main = "Variable Importance (Cross-Validated)")

# Select important features based on a threshold
important_vars <- rownames(importance_rf$importance)[importance_values > mean(importance_values)]
cat("Selected important variables:\n")
print(important_vars)


```

```{r}
# Load necessary libraries
library(data.table)
library(randomForest)
library(caret)
library(pROC)

# Load the dataset
RF_impute_df <- fread("RF_imputation_NEW.csv")

# Subset the data to include only selected important variables and outcome
important_features <- c("Renal_failure", "heart_rate", "temperature", "SP_O2", "Urine_output", "RBC", "Leucocyte", "Platelets", 
                        "Neutrophils", "Lymphocyte", "PT", "Creatinine", "Urea_nitrogen", "Blood_sodium", "Blood_calcium", 
                        "Anion_gap", "Magnesium_ion", "Bicarbonate", "Lactic_acid", "comorb_score", "outcome")
RF_simplified_df <- RF_impute_df[, ..important_features]

# Ensure the outcome variable has valid factor levels
RF_simplified_df$outcome <- factor(RF_simplified_df$outcome, levels = unique(RF_simplified_df$outcome), labels = make.names(unique(RF_simplified_df$outcome)))

# Prepare the predictor matrix and outcome variable
x_simplified <- as.matrix(RF_simplified_df[, -"outcome", with = FALSE])
y_simplified <- RF_simplified_df$outcome

# Set seed for reproducibility
set.seed(510)

# Define cross-validation control
train_control <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = "final"
)

# Train the Random Forest model using cross-validation with the simplified dataset
rf_cv_model_simplified <- train(
  x = x_simplified, 
  y = y_simplified,
  method = "rf",
  trControl = train_control,
  metric = "ROC",
  importance = TRUE,
  ntree = 500,  # Number of trees
  nodesize = 10,  # Minimum size of terminal nodes
  tuneGrid = expand.grid(mtry = 10)  # Number of variables randomly sampled as candidates at each split
)

# Print the results of the cross-validation
print(rf_cv_model_simplified)

# Extract the cross-validated AUC
auc_simplified <- rf_cv_model_simplified$results$ROC[rf_cv_model_simplified$results$ROC == max(rf_cv_model_simplified$results$ROC)]
cat("Cross-validated AUC for simplified model:", auc_simplified, "\n")

# Predict on the test data (if you have a separate test set)
# Here we use the cross-validated predictions
predictions_simplified <- rf_cv_model_simplified$pred$pred
probs_simplified <- rf_cv_model_simplified$pred[, make.names(levels(y_simplified)[2])]  # Assuming the positive class is the second level

# Calculate log-loss
log_loss <- function(y_true, y_prob) {
  epsilon <- 1e-15
  y_prob <- pmax(epsilon, pmin(1 - epsilon, y_prob))
  -mean(y_true * log(y_prob) + (1 - y_true) * log(1 - y_prob))
}

# Convert factor levels to 0 and 1 for log-loss calculation
y_true_simplified <- ifelse(rf_cv_model_simplified$pred$obs == levels(y_simplified)[2], 1, 0)
log_loss_value_simplified <- log_loss(y_true_simplified, probs_simplified)
cat("Cross-validated log-loss for simplified model:", log_loss_value_simplified, "\n")

# Extract feature importances for the simplified model
importance_rf_simplified <- varImp(rf_cv_model_simplified, scale = FALSE)

# Print feature importances for the simplified model
cat("Feature importances from Random Forest (simplified model):\n")
print(importance_rf_simplified)

# Plot feature importances for the simplified model
plot(importance_rf_simplified, main = "Variable Importance (Cross-Validated, Simplified Model)")

```













```{r}
# Verify no data leakage between training and validation sets
train_idx <- createDataPartition(y_rf, p = 0.8, list = FALSE)
train_data <- RF_complete_df[train_idx, ]
test_data <- RF_complete_df[-train_idx, ]

# Check overlap
intersect(rownames(train_data), rownames(test_data))

```






```{r}
# Load necessary libraries
library(data.table)
library(caret)

# Load the dataset
RF_impute_df <- fread("RF_imputation_NEW.csv")

# Subset the data to exclude unnecessary columns
RF_complete_df <- subset(RF_impute_df, select = -c(deathtime, survival_time, LOS, Unnamed_0, V1, admittime, ID, group, tLOS))

# Ensure the outcome variable is included and properly named
if (!("outcome" %in% names(RF_complete_df))) {
  stop("The 'outcome' column does not exist in the dataframe.")
}

# Ensure the outcome variable has valid factor levels
RF_complete_df$outcome <- factor(RF_complete_df$outcome, levels = unique(RF_complete_df$outcome), labels = make.names(unique(RF_complete_df$outcome)))

# Add unique row identifiers
RF_complete_df$rn <- seq_len(nrow(RF_complete_df))

# Properly partition data to avoid leakage
set.seed(783)
train_idx <- createDataPartition(RF_complete_df$outcome, p = 0.8, list = FALSE)
train_data <- RF_complete_df[train_idx, ]
test_data <- RF_complete_df[-train_idx, ]

# Ensure no overlap
if (length(intersect(train_data$rn, test_data$rn)) == 0) {
  cat("No data leakage detected.\n")
} else {
  stop("Data leakage detected.")
}

# Check the sizes of the training and test sets
cat("Training set size:", nrow(train_data), "\n")
cat("Test set size:", nrow(test_data), "\n")

# Remove the unique row identifiers
train_data <- train_data[, -c("rn")]
test_data <- test_data[, -c("rn")]

# Proceed with model training as previously described

```

```{r}
# Prepare the predictor matrix and outcome variable for training data
x_rf_train <- as.matrix(train_data[, setdiff(names(train_data), "outcome"), with = FALSE])
y_rf_train <- train_data$outcome

# Prepare the predictor matrix and outcome variable for test data
x_rf_test <- as.matrix(test_data[, setdiff(names(test_data), "outcome"), with = FALSE])
y_rf_test <- test_data$outcome

# Set seed for reproducibility
set.seed(783)

# Define cross-validation control
train_control <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = "final"
)

# Define the grid for hyperparameter tuning
tune_grid <- expand.grid(
  mtry = seq(2, 10, 2)  # Number of variables randomly sampled as candidates at each split
)

# Train the Random Forest model using cross-validation with the specified parameters
rf_cv_model <- train(
  x = x_rf_train, 
  y = y_rf_train,
  method = "rf",
  trControl = train_control,
  tuneGrid = tune_grid,
  metric = "ROC",
  importance = TRUE,
  ntree = 100,  # Reduce number of trees to prevent overfitting
  nodesize = 5  # Increase the minimum size of terminal nodes
)

# Print the results of the cross-validation
print(rf_cv_model)

# Evaluate the model on the test set
test_pred_probs <- predict(rf_cv_model, newdata = x_rf_test, type = "prob")[, 2]
test_roc <- roc(response = as.numeric(y_rf_test) - 1, predictor = test_pred_probs)
test_auc <- test_roc$auc
cat("Test AUC:", test_auc, "\n")

# Confusion matrix on test set
test_pred_class <- ifelse(test_pred_probs > 0.5, 1, 0)
conf_matrix_test <- table(Predicted = test_pred_class, Actual = as.numeric(y_rf_test) - 1)
print(conf_matrix_test)

# Extract and plot feature importances
importance_rf <- varImp(rf_cv_model, scale = FALSE)
importance_values <- importance_rf$importance[, 1]  # Extract importance values

# Print feature importances
cat("Feature importances from Random Forest (cross-validated):\n")
print(importance_values)

# Plot feature importances
plot(importance_rf, main = "Variable Importance (Cross-Validated)")

# Generate learning curves
train_sizes <- seq(0.1, 1.0, by = 0.1)
train_scores <- numeric(length(train_sizes))
test_scores <- numeric(length(train_sizes))

for (i in seq_along(train_sizes)) {
  train_size <- train_sizes[i]
  train_idx <- createDataPartition(y_rf_train, p = train_size, list = FALSE)
  x_train_part <- x_rf_train[train_idx, ]
  y_train_part <- y_rf_train[train_idx]
  
  if (length(train_idx) > 0 && nrow(x_train_part) > 0 && nrow(x_rf_train[-train_idx, ]) > 0) {
    rf_cv_model_part <- train(
      x = x_train_part,
      y = y_train_part,
      method = "rf",
      trControl = train_control,
      tuneGrid = tune_grid,
      metric = "ROC",
      importance = TRUE,
      ntree = 100,  # Reduce number of trees to prevent overfitting
      nodesize = 5  # Increase the minimum size of terminal nodes
    )
    train_pred <- predict(rf_cv_model_part, newdata = x_train_part, type = "prob")[, 2]  # Probabilities for positive class
    test_pred <- predict(rf_cv_model_part, newdata = x_rf_train[-train_idx, ], type = "prob")[, 2]  # Probabilities for positive class

    # Calculate AUC for training and test sets
    train_scores[i] <- roc(response = as.numeric(y_train_part) - 1, predictor = train_pred)$auc
    test_scores[i] <- roc(response = as.numeric(y_rf_train[-train_idx]) - 1, predictor = test_pred)$auc
  } else {
    train_scores[i] <- NA
    test_scores[i] <- NA
  }
}

# Function to plot learning curves
plot_learning_curve <- function(train_sizes, train_scores, test_scores) {
  plot(train_sizes, train_scores, type = "o", col = "blue", ylim = range(c(train_scores, test_scores), na.rm = TRUE), xlab = "Training Size", ylab = "Score", main = "Learning Curves")
  lines(train_sizes, test_scores, type = "o", col = "red")
  legend("bottomright", legend = c("Train", "Validation"), col = c("blue", "red"), lty = 1)
}

# Plot the learning curves
plot_learning_curve(train_sizes, train_scores, test_scores)

cat("Training AUCs:", train_scores, "\n")
cat("Validation AUCs:", test_scores, "\n")

```


```{r}
# Load necessary libraries
library(data.table)
library(randomForest)
library(caret)
library(pROC)

# Load the dataset
RF_impute_df <- fread("RF_imputation_NEW.csv")

# Subset the data to exclude unnecessary columns
RF_complete_df <- subset(RF_impute_df, select = -c(deathtime, survival_time, LOS, Unnamed_0, V1, admittime, ID, group, tLOS))

# Ensure the outcome variable is included and properly named
if (!("outcome" %in% names(RF_complete_df))) {
  stop("The 'outcome' column does not exist in the dataframe.")
}

# Ensure the outcome variable has valid factor levels
RF_complete_df$outcome <- factor(RF_complete_df$outcome, levels = unique(RF_complete_df$outcome), labels = make.names(unique(RF_complete_df$outcome)))

# Add unique row identifiers
RF_complete_df$rn <- seq_len(nrow(RF_complete_df))

# Properly partition data to avoid leakage
set.seed(510)
train_idx <- createDataPartition(RF_complete_df$outcome, p = 0.8, list = FALSE)
train_data <- RF_complete_df[train_idx, ]
test_data <- RF_complete_df[-train_idx, ]

# Ensure no overlap
if (length(intersect(train_data$rn, test_data$rn)) == 0) {
  cat("No data leakage detected.\n")
} else {
  stop("Data leakage detected.")
}

# Check the sizes of the training and test sets
cat("Training set size:", nrow(train_data), "\n")
cat("Test set size:", nrow(test_data), "\n")

# Remove the unique row identifiers
train_data <- train_data[, -c("rn")]
test_data <- test_data[, -c("rn")]

# Prepare the predictor matrix and outcome variable for training data
x_rf_train <- as.matrix(train_data[, setdiff(names(train_data), "outcome"), with = FALSE])
y_rf_train <- train_data$outcome

# Prepare the predictor matrix and outcome variable for test data
x_rf_test <- as.matrix(test_data[, setdiff(names(test_data), "outcome"), with = FALSE])
y_rf_test <- test_data$outcome

# Set seed for reproducibility
set.seed(510)

# Define cross-validation control
train_control <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = "final"
)

# Define the grid for hyperparameter tuning
tune_grid <- expand.grid(
  mtry = seq(2, 10, 2)  # Number of variables randomly sampled as candidates at each split
)

# Train the Random Forest model using cross-validation with the specified parameters
rf_cv_model <- train(
  x = x_rf_train, 
  y = y_rf_train,
  method = "rf",
  trControl = train_control,
  tuneGrid = tune_grid,
  metric = "ROC",
  importance = TRUE,
  ntree = 50,  # Further reduce number of trees to prevent overfitting
  nodesize = 20,  # Further increase the minimum size of terminal nodes
  maxnodes = 30  # Ensure maxnodes is greater than nodesize
)

# Print the results of the cross-validation
print(rf_cv_model)

# Evaluate the model on the test set
test_pred_probs <- predict(rf_cv_model, newdata = x_rf_test, type = "prob")[, 2]
test_roc <- roc(response = as.numeric(y_rf_test) - 1, predictor = test_pred_probs)
test_auc <- test_roc$auc
cat("Test AUC:", test_auc, "\n")

# Confusion matrix on test set
test_pred_class <- ifelse(test_pred_probs > 0.5, 1, 0)
conf_matrix_test <- table(Predicted = test_pred_class, Actual = as.numeric(y_rf_test) - 1)
print(conf_matrix_test)

# Extract and plot feature importances
importance_rf <- varImp(rf_cv_model, scale = FALSE)
importance_values <- importance_rf$importance[, 1]  # Extract importance values

# Print feature importances
cat("Feature importances from Random Forest (cross-validated):\n")
print(importance_values)

# Plot feature importances
plot(importance_rf, main = "Variable Importance (Cross-Validated)")

# Generate learning curves
train_sizes <- seq(0.1, 1.0, by = 0.1)
train_scores <- numeric(length(train_sizes))
test_scores <- numeric(length(train_sizes))

for (i in seq_along(train_sizes)) {
  train_size <- train_sizes[i]
  train_idx <- createDataPartition(y_rf_train, p = train_size, list = FALSE)
  x_train_part <- x_rf_train[train_idx, ]
  y_train_part <- y_rf_train[train_idx]
  
  if (length(train_idx) > 0 && nrow(x_train_part) > 0 && nrow(x_rf_train[-train_idx, ]) > 0) {
    rf_cv_model_part <- train(
      x = x_train_part,
      y = y_train_part,
      method = "rf",
      trControl = train_control,
      tuneGrid = tune_grid,
      metric = "ROC",
      importance = TRUE,
      ntree = 100,  # Further reduce number of trees to prevent overfitting
      nodesize = 5,  # Further increase the minimum size of terminal nodes
      maxnodes = 30  # Ensure maxnodes is greater than nodesize
    )
    train_pred <- predict(rf_cv_model_part, newdata = x_train_part, type = "prob")[, 2]  # Probabilities for positive class
    test_pred <- predict(rf_cv_model_part, newdata = x_rf_train[-train_idx, ], type = "prob")[, 2]  # Probabilities for positive class

    # Calculate AUC for training and test sets
    train_scores[i] <- roc(response = as.numeric(y_train_part) - 1, predictor = train_pred)$auc
    test_scores[i] <- roc(response = as.numeric(y_rf_train[-train_idx]) - 1, predictor = test_pred)$auc
  } else {
    train_scores[i] <- NA
    test_scores[i] <- NA
  }
}

# Function to plot learning curves
plot_learning_curve <- function(train_sizes, train_scores, test_scores) {
  plot(train_sizes, train_scores, type = "o", col = "blue", ylim = range(c(train_scores, test_scores), na.rm = TRUE), xlab = "Training Size", ylab = "Score", main = "Learning Curves")
  lines(train_sizes, test_scores, type = "o", col = "red")
  legend("bottomright", legend = c("Train", "Validation"), col = c("blue", "red"), lty = 1)
}

# Plot the learning curves
plot_learning_curve(train_sizes, train_scores, test_scores)

cat("Training AUCs:", train_scores, "\n")
cat("Validation AUCs:", test_scores, "\n")

```



```{r}
# Load necessary libraries
library(data.table)
library(randomForest)
library(caret)
library(pROC)

# Load the dataset
RF_impute_df <- fread("RF_imputation_NEW.csv")

# Subset the data to exclude unnecessary columns
RF_complete_df <- subset(RF_impute_df, select = -c(deathtime, survival_time, LOS, Unnamed_0, V1, admittime, ID, group, tLOS))

# Ensure the outcome variable is included and properly named
if (!("outcome" %in% names(RF_complete_df))) {
  stop("The 'outcome' column does not exist in the dataframe.")
}

# Ensure the outcome variable has valid factor levels
RF_complete_df$outcome <- factor(RF_complete_df$outcome, levels = unique(RF_complete_df$outcome), labels = make.names(unique(RF_complete_df$outcome)))

# Add unique row identifiers
RF_complete_df$rn <- seq_len(nrow(RF_complete_df))

# Properly partition data to avoid leakage
set.seed(510)
train_idx <- createDataPartition(RF_complete_df$outcome, p = 0.8, list = FALSE)
train_data <- RF_complete_df[train_idx, ]
test_data <- RF_complete_df[-train_idx, ]

# Ensure no overlap
if (length(intersect(train_data$rn, test_data$rn)) == 0) {
  cat("No data leakage detected.\n")
} else {
  stop("Data leakage detected.")
}

# Check the sizes of the training and test sets
cat("Training set size:", nrow(train_data), "\n")
cat("Test set size:", nrow(test_data), "\n")

# Remove the unique row identifiers
train_data <- train_data[, -c("rn")]
test_data <- test_data[, -c("rn")]

# Prepare the predictor matrix and outcome variable for training data
x_rf_train <- as.matrix(train_data[, setdiff(names(train_data), "outcome"), with = FALSE])
y_rf_train <- train_data$outcome

# Prepare the predictor matrix and outcome variable for test data
x_rf_test <- as.matrix(test_data[, setdiff(names(test_data), "outcome"), with = FALSE])
y_rf_test <- test_data$outcome

# Set seed for reproducibility
set.seed(510)

# Define cross-validation control
train_control <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = "final"
)

# Train the Random Forest model with more trees and wider hyperparameter tuning
rf_cv_model <- train(
  x = x_rf_train, 
  y = y_rf_train,
  method = "rf",
  trControl = train_control,
  tuneLength = 10,
  metric = "ROC",
  importance = TRUE,
  ntree = 500,  # Increased number of trees
  nodesize = 10,  # Increased minimum size of terminal nodes for regularization
  maxnodes = 20  # Reduced maximum number of terminal nodes for regularization
)

# Print the results of the cross-validation
print(rf_cv_model)

# Evaluate the model on the test set
test_pred_probs <- predict(rf_cv_model, newdata = x_rf_test, type = "prob")[, 2]
test_roc <- roc(response = as.numeric(y_rf_test) - 1, predictor = test_pred_probs)
test_auc <- test_roc$auc
cat("Test AUC:", test_auc, "\n")

# Confusion matrix on test set
test_pred_class <- ifelse(test_pred_probs > 0.5, 1, 0)
conf_matrix_test <- table(Predicted = test_pred_class, Actual = as.numeric(y_rf_test) - 1)
print(conf_matrix_test)

# Extract and plot feature importances
importance_rf <- varImp(rf_cv_model, scale = FALSE)
importance_values <- importance_rf$importance[, 1]  # Extract importance values

# Print feature importances
cat("Feature importances from Random Forest (cross-validated):\n")
print(importance_values)

# Plot feature importances
plot(importance_rf, main = "Variable Importance (Cross-Validated)")

# Generate learning curves
train_sizes <- seq(0.1, 1.0, by = 0.1)
train_scores <- numeric(length(train_sizes))
test_scores <- numeric(length(train_sizes))

for (i in seq_along(train_sizes)) {
  train_size <- train_sizes[i]
  train_idx <- createDataPartition(y_rf_train, p = train_size, list = FALSE)
  x_train_part <- x_rf_train[train_idx, ]
  y_train_part <- y_rf_train[train_idx]
  
  if (length(train_idx) > 0 && nrow(x_train_part) > 0 && nrow(x_rf_train[-train_idx, ]) > 0) {
    rf_cv_model_part <- train(
      x = x_train_part,
      y = y_train_part,
      method = "rf",
      trControl = train_control,
      tuneLength = 10,
      metric = "ROC",
      importance = TRUE,
      ntree = 500,  # Increased number of trees
      nodesize = 10,  # Increased minimum size of terminal nodes
      maxnodes = 20  # Reduced maximum number of terminal nodes
    )
    train_pred <- predict(rf_cv_model_part, newdata = x_train_part, type = "prob")[, 2]  # Probabilities for positive class
    test_pred <- predict(rf_cv_model_part, newdata = x_rf_train[-train_idx, ], type = "prob")[, 2]  # Probabilities for positive class

    # Calculate AUC for training and test sets
    train_scores[i] <- roc(response = as.numeric(y_train_part) - 1, predictor = train_pred)$auc
    test_scores[i] <- roc(response = as.numeric(y_rf_train[-train_idx]) - 1, predictor = test_pred)$auc
  } else {
    train_scores[i] <- NA
    test_scores[i] <- NA
  }
}

# Function to plot learning curves
plot_learning_curve <- function(train_sizes, train_scores, test_scores) {
  plot(train_sizes, train_scores, type = "o", col = "blue", ylim = range(c(train_scores, test_scores), na.rm = TRUE), xlab = "Training Size", ylab = "Score", main = "Learning Curves")
  lines(train_sizes, test_scores, type = "o", col = "red")
  legend("bottomright", legend = c("Train", "Validation"), col = c("blue", "red"), lty = 1)
}

# Plot the learning curves
plot_learning_curve(train_sizes, train_scores, test_scores)

cat("Training AUCs:", train_scores, "\n")
cat("Validation AUCs:", test_scores, "\n")
```



