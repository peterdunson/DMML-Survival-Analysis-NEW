---
title: "BART"
author: "Peter"
date: "`r Sys.Date()`"
output: html_document
---


```{r}
library(BART)
```


improved BART?
```{r}
# Load necessary libraries
library(BART)
library(caret)
library(pROC)
library(data.table)

# Load the dataset
RF_impute_df <- fread("RF_imputation_NEW.csv")

# Subset the data to exclude unnecessary columns and ensure the outcome variable is included and properly named
RF_complete_df <- subset(RF_impute_df, select = -c(deathtime, survival_time, LOS, Unnamed_0, V1, admittime, ID, group, tLOS, subject_id))

# Ensure the outcome variable has valid factor levels
RF_complete_df$outcome <- as.factor(RF_complete_df$outcome)

# Split the dataset into training and test sets
set.seed(403)
train_idx <- createDataPartition(RF_complete_df$outcome, p = 0.8, list = FALSE)
train_data <- RF_complete_df[train_idx, ]
test_data <- RF_complete_df[-train_idx, ]

# Define predictor matrix and outcome variable for training data
x_train <- as.matrix(train_data[, setdiff(names(train_data), "outcome"), with = FALSE])
y_train <- as.numeric(as.character(train_data$outcome))  # Convert outcome to numeric for BART

# Define predictor matrix and outcome variable for test data
x_test <- as.matrix(test_data[, setdiff(names(test_data), "outcome"), with = FALSE])
y_test <- as.numeric(as.character(test_data$outcome))  # Convert outcome to numeric for BART

# Fit the BART model for binary outcome
set.seed(403)
bart_model <- pbart(x.train = x_train, y.train = y_train, x.test = x_test)

# Predict on the training data
train_pred_probs <- bart_model$prob.train.mean
train_pred_class <- ifelse(train_pred_probs > 0.5, 1, 0)

# Predict on the test data
test_pred_probs <- bart_model$prob.test.mean
test_pred_class <- ifelse(test_pred_probs > 0.5, 1, 0)

# Function to calculate and print performance metrics
evaluate_performance <- function(y_true, y_prob, pred_class, dataset_name) {
  roc_obj <- roc(y_true, y_prob)
  auc <- roc_obj$auc
  log_loss <- function(y_true, y_prob) {
    epsilon <- 1e-15
    y_prob <- pmax(epsilon, pmin(1 - epsilon, y_prob))
    -mean(y_true * log(y_prob) + (1 - y_true) * log(1 - y_prob))
  }
  log_loss_value <- log_loss(y_true, y_prob)
  conf_matrix <- table(Predicted = pred_class, Actual = y_true)
  
  cat("\n", dataset_name, " Performance Metrics:\n", sep="")
  cat("AUC:", auc, "\n")
  cat("Log-loss:", log_loss_value, "\n")
  cat("Confusion Matrix:\n")
  print(conf_matrix)
  
  list(auc = auc, log_loss = log_loss_value, conf_matrix = conf_matrix)
}

# Evaluate performance on the training set
train_performance <- evaluate_performance(y_train, train_pred_probs, train_pred_class, "Training Set")

# Evaluate performance on the test set
test_performance <- evaluate_performance(y_test, test_pred_probs, test_pred_class, "Test Set")



```

```{r}
# Load necessary libraries
library(BART)
library(caret)
library(pROC)
library(data.table)

# Load the dataset
RF_impute_df <- fread("RF_imputation_NEW.csv")

# Subset the data to exclude unnecessary columns and ensure the outcome variable is included and properly named
RF_complete_df <- subset(RF_impute_df, select = -c(deathtime, survival_time, LOS, Unnamed_0, V1, admittime, ID, group, tLOS, Anion_gap, subject_id))

# Ensure the outcome variable has valid factor levels
RF_complete_df$outcome <- as.factor(RF_complete_df$outcome)

# Split the dataset into training and test sets
set.seed(403)
train_idx <- createDataPartition(RF_complete_df$outcome, p = 0.8, list = FALSE)
train_data <- RF_complete_df[train_idx, ]
test_data <- RF_complete_df[-train_idx, ]

# Define predictor matrix and outcome variable for training data
x_train <- as.matrix(train_data[, setdiff(names(train_data), "outcome"), with = FALSE])
y_train <- as.numeric(as.character(train_data$outcome))  # Convert outcome to numeric for BART

# Define predictor matrix and outcome variable for test data
x_test <- as.matrix(test_data[, setdiff(names(test_data), "outcome"), with = FALSE])
y_test <- as.numeric(as.character(test_data$outcome))  # Convert outcome to numeric for BART

# Fit the BART model for binary outcome
set.seed(403)
bart_model <- pbart(x.train = x_train, y.train = y_train, x.test = x_test)

# Predict on the training data
train_pred_probs <- bart_model$prob.train.mean
train_pred_class <- ifelse(train_pred_probs > 0.5, 1, 0)

# Predict on the test data
test_pred_probs <- bart_model$prob.test.mean
test_pred_class <- ifelse(test_pred_probs > 0.5, 1, 0)

# Function to calculate and print performance metrics
evaluate_performance <- function(y_true, y_prob, pred_class, dataset_name) {
  roc_obj <- roc(y_true, y_prob)
  auc <- roc_obj$auc
  log_loss <- function(y_true, y_prob) {
    epsilon <- 1e-15
    y_prob <- pmax(epsilon, pmin(1 - epsilon, y_prob))
    -mean(y_true * log(y_prob) + (1 - y_true) * log(1 - y_prob))
  }
  log_loss_value <- log_loss(y_true, y_prob)
  conf_matrix <- table(Predicted = pred_class, Actual = y_true)
  
  cat("\n", dataset_name, " Performance Metrics:\n", sep="")
  cat("AUC:", auc, "\n")
  cat("Log-loss:", log_loss_value, "\n")
  cat("Confusion Matrix:\n")
  print(conf_matrix)
  
  list(auc = auc, log_loss = log_loss_value, conf_matrix = conf_matrix)
}

# Evaluate performance on the training set
train_performance <- evaluate_performance(y_train, train_pred_probs, train_pred_class, "Training Set")

# Evaluate performance on the test set
test_performance <- evaluate_performance(y_test, test_pred_probs, test_pred_class, "Test Set")

# Function to calculate permutation importance
permutation_importance <- function(model, x, y, metric, n_repeats = 5) {
  original_metric <- metric(y, predict(model, newdata = x))
  importance <- rep(0, ncol(x))
  
  for (i in seq_along(importance)) {
    for (j in 1:n_repeats) {
      x_permuted <- x
      x_permuted[, i] <- sample(x_permuted[, i])
      permuted_metric <- metric(y, predict(model, newdata = x_permuted))
      importance[i] <- importance[i] + (original_metric - permuted_metric)
    }
    importance[i] <- importance[i] / n_repeats
  }
  
  importance
}

# Calculate AUC metric for permutation importance
auc_metric <- function(y_true, y_prob) {
  roc_obj <- roc(y_true, y_prob)
  auc(roc_obj)
}


```



```{r}
RF_impute_df <- fread("RF_imputation_NEW.csv")
RF_complete_df <- subset(RF_impute_df, select = -c(deathtime, survival_time, LOS, Unnamed_0, V1, admittime, ID, group, tLOS, Anion_gap))

# Load necessary libraries
library(BART)
library(data.table)
library(pROC)

# Load and preprocess the data
dt <- as.data.table(RF_complete_df)

# Print the structure of the entire dataframe
cat("Structure of the entire dataframe:\n")
print(str(dt))

# Exclude the 'outcome' column and prepare the predictor matrix
if ("outcome" %in% names(dt)) {
  predictor_names <- setdiff(names(dt), "outcome")  # Get names of predictor columns
  x <- as.matrix(dt[, ..predictor_names])  # Exclude the outcome variable
  y <- as.numeric(dt$outcome)  # Convert outcome to numeric
} else {
  stop("The 'outcome' column does not exist in the dataframe.")
}

# Print column names after excluding 'outcome'
cat("Column names after excluding 'outcome':\n")
print(colnames(x))

# Ensure x has at least two columns
if (ncol(x) < 2) {
  stop("The predictor matrix 'x' must have at least two columns.")
}

# Set seed for reproducibility
set.seed(213)

# Split the data into training and test sets
train_idx <- sample(1:nrow(x), size = 0.8 * nrow(x))
x_train <- x[train_idx, ]
y_train <- y[train_idx]
x_test <- x[-train_idx, ]
y_test <- y[-train_idx]

# Train a BART model for feature selection
bart_model <- pbart(x.train = x_train, y.train = y_train, x.test = x_test)

# Check the structure of bart_model to find the variable importance
cat("Structure of bart_model:\n")
print(str(bart_model))

# Determine the variable inclusion proportions correctly
var_importance <- colMeans(bart_model$varcount)
selected_features <- names(var_importance)[which(var_importance > 0.5)]  # Arbitrary threshold, adjust as needed
cat("Selected features based on BART:\n")
print(selected_features)

# Subset the training and test data to the selected features
x_train_selected <- x_train[, selected_features, drop = FALSE]
x_test_selected <- x_test[, selected_features, drop = FALSE]

# Debugging: Print dimensions of x_train_selected and x_train
cat("Dimensions of x_train_selected:\n")
print(dim(x_train_selected))
cat("Dimensions of x_train:\n")
print(dim(x_train))

# Predict on the training data
train_predictions <- bart_model$prob.train.mean

# Ensure the predictions and labels are aligned
if (length(train_predictions) != length(y_train)) {
  stop("Length of training predictions does not match length of y_train")
}

# Function to calculate log-loss
log_loss <- function(y_true, y_pred) {
  epsilon <- 1e-15
  y_pred <- pmin(pmax(y_pred, epsilon), 1 - epsilon)
  -mean(y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))
}

# Calculate log-loss and AUC on the training data
train_log_loss <- log_loss(y_train, train_predictions)
cat("Train log-loss:", train_log_loss, "\n")
train_roc <- roc(y_train, train_predictions)
train_auc <- train_roc$auc
cat("Train AUC:", train_auc, "\n")

# Predict on the test data
test_predictions <- bart_model$prob.test.mean

# Ensure the predictions and labels are aligned
if (length(test_predictions) != length(y_test)) {
  stop("Length of test predictions does not match length of y_test")
}

# Calculate log-loss and AUC on the test data
test_log_loss <- log_loss(y_test, test_predictions)
cat("Test log-loss:", test_log_loss, "\n")
test_roc <- roc(y_test, test_predictions)
test_auc <- test_roc$auc
cat("Test AUC:", test_auc, "\n")

# Plot ROC curve for training and test data
plot(train_roc, main = "ROC Curve (Training Data)")
plot(test_roc, add = TRUE, col = "red")
legend("bottomright", legend = c("Train", "Test"), col = c("black", "red"), lty = 1)

# Print variable importance
cat("Variable Importance:\n")
print(var_importance)

# Identify the selected covariates (based on importance threshold)
selected_covariates <- names(var_importance)[var_importance > 0.5]
cat("Selected covariates:", selected_covariates, "\n")


```

















