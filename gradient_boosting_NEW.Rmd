---
title: "gradient_boosting"
author: "Peter"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
# Load necessary libraries

library(xgboost)
library(data.table)
library(caret)

# Load the dataset
RF_impute_df <- fread("RF_imputation_NEW.csv")

# Subset the data to exclude unnecessary columns and ensure the outcome variable is included and properly named
RF_complete_df <- subset(RF_impute_df, select = -c(deathtime, survival_time, LOS, Unnamed_0, V1, admittime, ID, group, tLOS))

# Ensure the outcome variable has valid factor levels
RF_complete_df$outcome <- factor(RF_complete_df$outcome, levels = unique(RF_complete_df$outcome))

# Prepare the predictor matrix and outcome variable
x <- as.matrix(RF_complete_df[, setdiff(names(RF_complete_df), "outcome"), with = FALSE])
y <- as.numeric(RF_complete_df$outcome) - 1  # Convert outcome to 0 and 1

# Split the dataset into training and test sets
set.seed(510)
train_idx <- createDataPartition(y, p = 0.7, list = FALSE)
x_train <- x[train_idx, ]
y_train <- y[train_idx]
x_test <- x[-train_idx, ]
y_test <- y[-train_idx]

# Create xgboost DMatrix
dtrain <- xgb.DMatrix(data = x_train, label = y_train)
dtest <- xgb.DMatrix(data = x_test, label = y_test)

# Set parameters for xgboost
params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  max_depth = 2,
  eta = 0.15,
  nthread = 2
)

# Train the model
xgb_model <- xgb.train(params, dtrain, nrounds = 100, watchlist = list(eval = dtest, train = dtrain), verbose = 1)

# Get feature importance
importance_matrix <- xgb.importance(feature_names = colnames(x_train), model = xgb_model)
print(importance_matrix)

# Plot feature importance
xgb.plot.importance(importance_matrix, main = "Feature Importance (Gradient Boosting)")

# Evaluate model performance
pred <- predict(xgb_model, newdata = x_test)
auc <- roc(y_test, pred)$auc
cat("AUC:", auc, "\n")

# Calculate log-loss
log_loss <- function(y_true, y_prob) {
  epsilon <- 1e-15
  y_prob <- pmax(epsilon, pmin(1 - epsilon, y_prob))
  -mean(y_true * log(y_prob) + (1 - y_true) * log(1 - y_prob))
}
log_loss_value <- log_loss(y_test, pred)
cat("Log-loss:", log_loss_value, "\n")


```


```{r}
# Load necessary libraries
library(xgboost)
library(data.table)
library(caret)
library(pROC)

# Load the dataset
RF_impute_df <- fread("RF_imputation_NEW.csv")

# Subset the data to exclude unnecessary columns and ensure the outcome variable is included and properly named
RF_complete_df <- subset(RF_impute_df, select = -c(deathtime, survival_time, LOS, Unnamed_0, V1, admittime, ID, group, tLOS))

# Ensure the outcome variable has valid factor levels
RF_complete_df$outcome <- factor(RF_complete_df$outcome, levels = unique(RF_complete_df$outcome))

# Prepare the predictor matrix and outcome variable
x <- as.matrix(RF_complete_df[, setdiff(names(RF_complete_df), "outcome"), with = FALSE])
y <- as.numeric(RF_complete_df$outcome) - 1  # Convert outcome to 0 and 1

# Split the dataset into training and test sets
set.seed(510)
train_idx <- createDataPartition(y, p = 0.7, list = FALSE)
x_train <- x[train_idx, ]
y_train <- y[train_idx]
x_test <- x[-train_idx, ]
y_test <- y[-train_idx]

# Create xgboost DMatrix
dtrain <- xgb.DMatrix(data = x_train, label = y_train)
dtest <- xgb.DMatrix(data = x_test, label = y_test)

# Set parameters for xgboost
params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  max_depth = 2,
  eta = 0.15,
  nthread = 2
)

# Perform cross-validation to find the best number of rounds
cv <- xgb.cv(params, dtrain, nrounds = 100, nfold = 10, metrics = "auc", stratified = TRUE, verbose = 1)

# Print the structure of the cv object to understand its content
str(cv)

# Manually identify the best number of rounds based on the evaluation metric
best_nrounds <- which.max(cv$evaluation_log$test_auc_mean)
cat("Best number of rounds:", best_nrounds, "\n")

# Train the final model using the best number of rounds
xgb_model <- xgb.train(params, dtrain, nrounds = best_nrounds, watchlist = list(eval = dtest, train = dtrain), verbose = 1)

# Get feature importance
importance_matrix <- xgb.importance(feature_names = colnames(x_train), model = xgb_model)
print(importance_matrix)

# Plot feature importance
xgb.plot.importance(importance_matrix, main = "Feature Importance (Gradient Boosting)")

# Predict on the test data
pred <- predict(xgb_model, newdata = x_test)

# Calculate AUC
roc_obj <- roc(y_test, pred)
auc <- roc_obj$auc
cat("AUC:", auc, "\n")

# Plot ROC curve
plot.roc(roc_obj, main = "ROC Curve (Test Data)")

# Calculate log-loss
log_loss <- function(y_true, y_prob) {
  epsilon <- 1e-15
  y_prob <- pmax(epsilon, pmin(1 - epsilon, y_prob))
  -mean(y_true * log(y_prob) + (1 - y_true) * log(1 - y_prob))
}
log_loss_value <- log_loss(y_test, pred)
cat("Log-loss:", log_loss_value, "\n")

# Confusion matrix
pred_class <- ifelse(pred > 0.5, 1, 0)
conf_matrix <- table(Predicted = pred_class, Actual = y_test)
print(conf_matrix)

# Function to plot learning curves
plot_learning_curve <- function(train_sizes, train_scores, test_scores) {
  plot(train_sizes, train_scores, type = "o", col = "blue", ylim = range(c(train_scores, test_scores)), xlab = "Training Size", ylab = "Score", main = "Learning Curves")
  lines(train_sizes, test_scores, type = "o", col = "red")
  legend("bottomright", legend = c("Train", "Validation"), col = c("blue", "red"), lty = 1)
}

# Generate learning curves
train_sizes <- seq(0.1, 1.0, by = 0.1)
train_scores <- numeric(length(train_sizes))
test_scores <- numeric(length(train_sizes))

for (i in seq_along(train_sizes)) {
  train_size <- train_sizes[i]
  train_idx <- createDataPartition(y_train, p = train_size, list = FALSE)
  x_train_part <- x_train[train_idx, ]
  y_train_part <- y_train[train_idx]
  dtrain_part <- xgb.DMatrix(data = x_train_part, label = y_train_part)
  
  model <- xgb.train(params, dtrain_part, nrounds = best_nrounds, verbose = 0)
  train_pred <- predict(model, newdata = x_train_part)
  test_pred <- predict(model, newdata = x_test)
  
  # Ensure no NA values in predictions
  if (anyNA(train_pred) || anyNA(test_pred)) {
    stop("Predictions contain NA values.")
  }
  
  # Ensure lengths match
  if (length(y_train_part) != length(train_pred)) {
    stop("Length of y_train_part and train_pred do not match.")
  }
  if (length(y_test) != length(test_pred)) {
    stop("Length of y_test and test_pred do not match.")
  }
  
  # Calculate AUC for training and test sets
  train_scores[i] <- roc(y_train_part, train_pred)$auc
  test_scores[i] <- roc(y_test, test_pred)$auc
}

# Plot learning curves
plot_learning_curve(train_sizes, train_scores, test_scores)


```




