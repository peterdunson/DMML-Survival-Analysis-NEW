---
title: "Elastic Net Inference Nate"
author: "Nate"
date: "2024-07-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load necessary libraries
library(glmnet)
library(data.table)
library(pROC)
library(boot)

# Load the dataset
RF_impute_df <- fread("RF_imputation_NEW.csv")

# Subset the data to exclude unnecessary columns and ensure the outcome variable is included and properly named
RF_complete_df <- subset(RF_impute_df, select = -c(deathtime, survival_time, LOS, Unnamed_0, V1, admittime, ID, group, tLOS, Anion_gap, subject_id))

# Convert data.frame to data.table
dt <- as.data.table(RF_complete_df)

# Print the structure of the entire dataframe
cat("Structure of the entire dataframe:\n")
print(str(dt))

# Exclude the 'outcome' column and prepare the predictor matrix
if ("outcome" %in% names(dt)) {
  predictor_names <- setdiff(names(dt), "outcome")  # Get names of predictor columns
  x <- as.matrix(dt[, ..predictor_names])  # Exclude the outcome variable
  y <- as.numeric(dt$outcome)  # Convert outcome to numeric vector
} else {
  stop("The 'outcome' column does not exist in the dataframe.")
}

# Print column names after excluding 'outcome'
cat("Column names after excluding 'outcome':\n")
print(colnames(x))

# Ensure x has at least two columns
if (ncol(x) < 2) {
  stop("The predictor matrix 'x' must have at least two columns.")
}

# Set seed for reproducibility
set.seed(213)

# Split the data into training and test sets
train_idx <- sample(1:nrow(x), size = 0.8 * nrow(x))
x_train <- x[train_idx, ]
y_train <- y[train_idx]
x_test <- x[-train_idx, ]
y_test <- y[-train_idx]

# Define a range of alpha values to test
alpha_values <- seq(0, 1, by = 0.1)

# Initialize variables to store results
best_alpha <- NULL
best_lambda <- NULL
best_log_loss <- Inf
best_auc <- -Inf

# Function to calculate log-loss
log_loss <- function(y_true, y_pred) {
  -mean(y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))
}

# Iterate through each alpha value
for (alpha in alpha_values) {
  # Perform cross-validation for the current alpha value
  cv_fit <- cv.glmnet(x_train, y_train, family = "binomial", alpha = alpha)
  
  # Get the best lambda for the current alpha
  lambda_min <- cv_fit$lambda.min
  
  # Evaluate the model using custom cross-validation
  k <- 10  # Number of folds
  folds <- sample(rep(1:k, length.out = length(y_train)))  # Randomly assign data to folds
  
  cv_errors <- c()  # To store cross-validation errors
  auc_values <- c()  # To store cross-validation AUC values
  
  for (i in 1:k) {
    # Split the data into training and validation sets
    train_fold_idx <- which(folds != i)
    valid_fold_idx <- which(folds == i)
    
    x_train_fold <- x_train[train_fold_idx, ]
    y_train_fold <- y_train[train_fold_idx]
    x_valid_fold <- x_train[valid_fold_idx, ]
    y_valid_fold <- y_train[valid_fold_idx]
    
    # Fit the model on the training fold data
    model <- glmnet(x_train_fold, y_train_fold, family = "binomial", alpha = alpha, lambda = lambda_min)
    
    # Predict on the validation fold data
    predictions <- predict(model, s = lambda_min, newx = x_valid_fold, type = "response")
    predictions <- as.numeric(predictions)  # Ensure predictions are numeric vector
    
    # Calculate the validation error (log-loss)
    cv_errors <- c(cv_errors, log_loss(y_valid_fold, predictions))
    
    # Calculate the validation AUC
    auc <- roc(y_valid_fold, predictions)$auc
    auc_values <- c(auc_values, auc)
  }
  
  # Calculate the mean cross-validated log-loss and AUC for the current alpha
  mean_log_loss <- mean(cv_errors)
  mean_auc <- mean(auc_values)
  
  # Update the best alpha and lambda if the current log-loss is lower or AUC is higher
  if (mean_log_loss < best_log_loss) {
    best_log_loss <- mean_log_loss
    best_alpha <- alpha
    best_lambda <- lambda_min
  }
  
  if (mean_auc > best_auc) {
    best_auc <- mean_auc
    best_alpha <- alpha
    best_lambda <- lambda_min
  }
  
  cat("Alpha:", alpha, "Mean log-loss:", mean_log_loss, "Mean AUC:", mean_auc, "\n")
}

# Print the best alpha and lambda
cat("Best alpha:", best_alpha, "\n")
cat("Best lambda:", best_lambda, "\n")

# Fit the final model using the best alpha and lambda on the full training data
final_model <- glmnet(x_train, y_train, family = "binomial", alpha = best_alpha, lambda = best_lambda)

# Function to fit the model (for bootstrapping)
fit_model <- function(data, indices) {
  d <- data[indices, ]  # Allows boot to select sample
  x_boot <- as.matrix(d[, -ncol(d)])
  y_boot <- d[, ncol(d)]
  model <- glmnet(x_boot, y_boot, family = "binomial", alpha = best_alpha, lambda = best_lambda)
  return(as.vector(coef(model)))
}

# Prepare data for bootstrapping
boot_data <- cbind(x_train, y_train)

# Perform bootstrapping
set.seed(123)
boot_results <- boot(boot_data, statistic = fit_model, R = 1000)

# Calculate confidence intervals for coefficients
coef_summary <- boot.ci(boot_results, type = "bca")

# Print confidence intervals
print(coef_summary)

# Predict on the training data
train_predictions <- predict(final_model, s = best_lambda, newx = x_train, type = "response")
train_predictions <- as.numeric(train_predictions)  # Ensure predictions are numeric vector

# Calculate log-loss on the training data
train_log_loss <- log_loss(y_train, train_predictions)
cat("Train log-loss:", train_log_loss, "\n")

# Calculate AUC on the training data
train_roc <- roc(y_train, train_predictions)
train_auc <- train_roc$auc
cat("Train AUC:", train_auc, "\n")

# Predict on the test data
test_predictions <- predict(final_model, s = best_lambda, newx = x_test, type = "response")
test_predictions <- as.numeric(test_predictions)  # Ensure predictions are numeric vector

# Calculate log-loss on the test data
test_log_loss <- log_loss(y_test, test_predictions)
cat("Test log-loss:", test_log_loss, "\n")

# Calculate AUC on the test data
test_roc <- roc(y_test, test_predictions)
test_auc <- test_roc$auc
cat("Test AUC:", test_auc, "\n")

# Extract the coefficients from the final model
coefficients <- coef(final_model)
print(coefficients)

# Identify the selected covariates (non-zero coefficients)
selected_covariates <- rownames(coefficients)[which(coefficients != 0)]
cat("Selected covariates:", selected_covariates, "\n")

```

```{r}
# Load necessary libraries
library(glmnet)
library(data.table)
library(pROC)
library(boot)

# Load the dataset
RF_impute_df <- fread("RF_imputation_NEW.csv")

# Subset the data to exclude unnecessary columns and ensure the outcome variable is included and properly named
RF_complete_df <- subset(RF_impute_df, select = -c(deathtime, survival_time, LOS, Unnamed_0, V1, admittime, ID, group, tLOS, Anion_gap, subject_id))

# Convert data.frame to data.table
dt <- as.data.table(RF_complete_df)

# Print the structure of the entire dataframe
cat("Structure of the entire dataframe:\n")
print(str(dt))

# Exclude the 'outcome' column and prepare the predictor matrix
if ("outcome" %in% names(dt)) {
  predictor_names <- setdiff(names(dt), "outcome")  # Get names of predictor columns
  x <- as.matrix(dt[, ..predictor_names])  # Exclude the outcome variable
  y <- as.numeric(dt$outcome)  # Convert outcome to numeric vector
} else {
  stop("The 'outcome' column does not exist in the dataframe.")
}

# Print column names after excluding 'outcome'
cat("Column names after excluding 'outcome':\n")
print(colnames(x))

# Ensure x has at least two columns
if (ncol(x) < 2) {
  stop("The predictor matrix 'x' must have at least two columns.")
}

# Set seed for reproducibility
set.seed(213)

# Split the data into training and test sets
train_idx <- sample(1:nrow(x), size = 0.8 * nrow(x))
x_train <- x[train_idx, ]
y_train <- y[train_idx]
x_test <- x[-train_idx, ]
y_test <- y[-train_idx]

# Define a range of alpha values to test
alpha_values <- seq(0, 1, by = 0.1)

# Initialize variables to store results
best_alpha <- NULL
best_lambda <- NULL
best_log_loss <- Inf
best_auc <- -Inf

# Function to calculate log-loss
log_loss <- function(y_true, y_pred) {
  -mean(y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))
}

# Iterate through each alpha value
for (alpha in alpha_values) {
  # Perform cross-validation for the current alpha value
  cv_fit <- cv.glmnet(x_train, y_train, family = "binomial", alpha = alpha)
  
  # Get the best lambda for the current alpha
  lambda_min <- cv_fit$lambda.min
  
  # Evaluate the model using custom cross-validation
  k <- 10  # Number of folds
  folds <- sample(rep(1:k, length.out = length(y_train)))  # Randomly assign data to folds
  
  cv_errors <- c()  # To store cross-validation errors
  auc_values <- c()  # To store cross-validation AUC values
  
  for (i in 1:k) {
    # Split the data into training and validation sets
    train_fold_idx <- which(folds != i)
    valid_fold_idx <- which(folds == i)
    
    x_train_fold <- x_train[train_fold_idx, ]
    y_train_fold <- y_train[train_fold_idx]
    x_valid_fold <- x_train[valid_fold_idx, ]
    y_valid_fold <- y_train[valid_fold_idx]
    
    # Fit the model on the training fold data
    model <- glmnet(x_train_fold, y_train_fold, family = "binomial", alpha = alpha, lambda = lambda_min)
    
    # Predict on the validation fold data
    predictions <- predict(model, s = lambda_min, newx = x_valid_fold, type = "response")
    predictions <- as.numeric(predictions)  # Ensure predictions are numeric vector
    
    # Calculate the validation error (log-loss)
    cv_errors <- c(cv_errors, log_loss(y_valid_fold, predictions))
    
    # Calculate the validation AUC
    auc <- roc(y_valid_fold, predictions)$auc
    auc_values <- c(auc_values, auc)
  }
  
  # Calculate the mean cross-validated log-loss and AUC for the current alpha
  mean_log_loss <- mean(cv_errors)
  mean_auc <- mean(auc_values)
  
  # Update the best alpha and lambda if the current log-loss is lower or AUC is higher
  if (mean_log_loss < best_log_loss) {
    best_log_loss <- mean_log_loss
    best_alpha <- alpha
    best_lambda <- lambda_min
  }
  
  if (mean_auc > best_auc) {
    best_auc <- mean_auc
    best_alpha <- alpha
    best_lambda <- lambda_min
  }
  
  cat("Alpha:", alpha, "Mean log-loss:", mean_log_loss, "Mean AUC:", mean_auc, "\n")
}

# Print the best alpha and lambda
cat("Best alpha:", best_alpha, "\n")
cat("Best lambda:", best_lambda, "\n")

# Fit the final model using the best alpha and lambda on the full training data
final_model <- glmnet(x_train, y_train, family = "binomial", alpha = best_alpha, lambda = best_lambda)

# Function to fit the model (for bootstrapping)
fit_model <- function(data, indices) {
  d <- data[indices, ]  # Allows boot to select sample
  x_boot <- as.matrix(d[, -ncol(d)])
  y_boot <- d[, ncol(d)]
  model <- glmnet(x_boot, y_boot, family = "binomial", alpha = best_alpha, lambda = best_lambda)
  return(as.vector(coef(model)))
}

# Prepare data for bootstrapping
boot_data <- cbind(x_train, y_train)

# Perform bootstrapping
set.seed(123)
boot_results <- boot(boot_data, statistic = fit_model, R = 1000)

# Extract coefficients from final model
coefficients <- coef(final_model)

# Function to extract coefficients from bootstrapped models
boot_coefficients <- t(sapply(1:ncol(boot_results$t), function(i) {
  coef(boot_results$t[, i])
}))

# Calculate confidence intervals for each coefficient
coef_intervals <- apply(boot_coefficients, 2, function(coef_samples) {
  quantile(coef_samples, c(0.025, 0.975))  # 95% confidence interval
})

# Display coefficient names and their confidence intervals
coef_summary <- data.frame(
  Coefficient = rownames(coef_intervals),
  Lower_CI = coef_intervals[, 1],
  Upper_CI = coef_intervals[, 2]
)

print(coef_summary)

```

